{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 文章来源:https://zhuanlan.zhihu.com/p/279758736\n",
    "\n",
    "PyTorch是动态图，即计算图的搭建和运算是同时的，随时可以输出结果；而TensorFlow是静态图。\n",
    "\n",
    "## pytorch计算图\n",
    "在pytorch的计算图里只有两种元素：数据（tensor）和 运算（operation）\n",
    "\n",
    "1. 运算包括了：加减乘除、开方、幂指对、三角函数等可求导运算\n",
    "\n",
    "2. 数据可分为：叶子节点（leaf node）和非叶子节点；叶子节点是用户创建的节点，不依赖其它节点；它们表现出来的区别在于反向传播结束之后，非叶子节点的梯度会被释放掉，只保留叶子节点的梯度，这样就节省了内存。如果想要保留非叶子节点的梯度，可以使用retain_grad()方法。\n",
    "\n",
    "torch.tensor 具有如下属性：\n",
    "\n",
    "1. 查看 是否可以求导 requires_grad\n",
    "2. 查看 运算名称 grad_fn\n",
    "3. 查看 是否为叶子节点 is_leaf\n",
    "4. 查看 导数值 grad\n",
    "\n",
    "> 针对requires_grad属性，自己定义的叶子节点默认为False，而非叶子节点默认为True，神经网络中的权重默认为True。判断哪些节点是True/False的一个原则就是从你需要求导的叶子节点到loss节点之间是一条可求导的通路。\n",
    "\n",
    "当我们想要对某个Tensor变量求梯度时，需要先指定requires_grad属性为True，指定方式主要有两种："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.).requires_grad_() # 第一种\n",
    "\n",
    "x = torch.tensor(1., requires_grad=True) # 第二种"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch提供两种求梯度的方法：backward() 和 torch.autograd.grad() ，他们的区别在于前者是给叶子节点填充.grad字段，而后者是直接返回梯度给你，我会在后面举例说明。还需要知道y.backward()其实等同于torch.autograd.backward(y)\n",
    "\n",
    "# 案例\n",
    "## 案例1\n",
    "> 一个简单的求导例子是： $ y = (x + 1) * (x +2)$，计算 $\\partial y / \\partial x$ ，假设给定 $x = 2$, 先画出计算图\n",
    "\n",
    "\n",
    "![](https://files.mdnice.com/user/7098/de111e14-c839-4f16-b0e3-870480705a57.png)\n",
    "\n",
    "\n",
    "\n",
    "手算的话: \n",
    "$\n",
    "\\frac {\\partial y} {\\partial x} = \\frac {\\partial y} {\\partial a}\\frac {\\partial a} {\\partial x} + \\frac {\\partial y} {\\partial b}\\frac {\\partial b} {\\partial x} = (x + 2) * 1 + (x + 1) * 1 \\rightarrow 7\n",
    "$\n",
    "\n",
    "### 使用backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2., requires_grad=True)\n",
    "\n",
    "a = torch.add(x, 1)\n",
    "b = torch.add(x, 2)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "# >>>tensor(7.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "看一下这几个tensor的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad:  True True True True\n",
      "is_leaf:  True False False False\n",
      "grad:  tensor(7.) None None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuanz\\AppData\\Local\\Temp/ipykernel_9372/335890420.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n"
     ]
    }
   ],
   "source": [
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n",
    "\n",
    "# >>>requires_grad:  True True True True\n",
    "# >>>is_leaf:  True False False False\n",
    "# >>>grad:  tensor(7.) None None None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用backward()函数反向传播计算tensor的梯度时，并不计算所有tensor的梯度，而是只计算满足这几个条件的tensor的梯度：\n",
    "1. 类型为叶子节点、\n",
    "2. requires_grad=True、\n",
    "3. 依赖该tensor的所有tensor的requires_grad=True。\n",
    "\n",
    "所有满足条件的变量梯度会自动保存到对应的grad属性里。\n",
    "\n",
    "### 使用autograd.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2., requires_grad=True)\n",
    "\n",
    "a = torch.add(x, 1)\n",
    "b = torch.add(x, 2)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "grad = torch.autograd.grad(outputs=y, inputs=x)\n",
    "print(grad[0])\n",
    "# >>>tensor(7.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为制定了输入y,输入了x, 所以返回值就是$\\partial y / \\partial x$ 这一梯度，完整的返回值其实是一个元组，保留第一个元素就行，后面的元素可以不用管。\n",
    "\n",
    "## 案例2\n",
    "\n",
    "> 再举一个复杂一点且高阶求导的例子： $z = x^2 y$,计算$\\partial z / \\partial x, \\partial z /\\partial y, \\partial^2z / \\partial ^2x$,假设给定$x = 2, y = 3$\n",
    "\n",
    "手算的话:$\\partial z / \\partial x = 2xy = 12, \\partial z /\\partial y = x^2 = 4, \\partial^2z / \\partial ^2x = 2y = 6$\n",
    "\n",
    "### 求一阶导可以用backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.) tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2., requires_grad=True)\n",
    "y = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "z.backward()\n",
    "print(x.grad, y.grad)\n",
    "# >>>tensor(12.) tensor(4.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 也可以用autograd.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x)\n",
    "print(grad_x[0])\n",
    "# >>>tensor(12.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么不在这里面同时也求对y的导数呢？\n",
    "\n",
    "因为无论是backward还是autograd.grad在计算一次梯度后图就被释放了，如果想要保留，需要添加retain_graph=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.) tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, retain_graph=True)\n",
    "grad_y = torch.autograd.grad(outputs=z, inputs=y)\n",
    "\n",
    "print(grad_x[0], grad_y[0])\n",
    "# >>>tensor(12.) tensor(4.) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看如何求高阶导，理论上其实是上面的grad_x再对x求梯度，试一下看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9372/2722476632.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mgrad_x\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mz\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mgrad_xx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mgrad_x\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad_xx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Users\\yuanz\\anaconda3\\envs\\mynet\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mgrad\u001B[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001B[0m\n\u001B[0;32m    298\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_vmap_internals\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_vmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvjp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mallow_none_pass_through\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad_outputs_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    299\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 300\u001B[1;33m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[0;32m    301\u001B[0m             \u001B[0mt_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_outputs_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mt_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    302\u001B[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[1;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, retain_graph=True)\n",
    "grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)\n",
    "\n",
    "print(grad_xx[0])\n",
    "# >>>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "报错了，虽然retain_graph=True保留了计算图和中间变量梯度， 但没有保存grad_x的运算方式，需要使用creat_graph=True在保留原图的基础上再建立额外的求导计算图，也就是会把$\\partial z / \\partial x$这样的运算存下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# autograd.grad() + autograd.grad()\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, create_graph=True)\n",
    "grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)\n",
    "\n",
    "print(grad_xx[0])\n",
    "# >>>tensor(6.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grad_xx这里也可以直接用backward()，相当于直接从$\\partial z / \\partial x = 2xy$开始回传"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "# autograd.grad() + backward()\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad = torch.autograd.grad(outputs=z, inputs=x, create_graph=True)\n",
    "grad[0].backward()\n",
    "\n",
    "print(x.grad)\n",
    "# >>>tensor(6.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以先用backward()然后对x.grad这个一阶导继续求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuanz\\anaconda3\\envs\\mynet\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\engine.cpp:1064.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "# backward() + autograd.grad()\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "z.backward(create_graph=True)\n",
    "grad_xx = torch.autograd.grad(outputs=x.grad, inputs=x)\n",
    "\n",
    "print(grad_xx[0])\n",
    "# >>>tensor(6.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "那是不是也可以直接用两次backward()呢？第二次直接x.grad从开始回传，我们试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18., grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "# backward() + backward()\n",
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "z.backward(create_graph=True) # x.grad = 12\n",
    "x.grad.backward()\n",
    "\n",
    "print(x.grad)\n",
    "# >>>tensor(18., grad_fn=<CopyBackwards>)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现了问题，结果不是6，而是18，发现第一次回传时输出x梯度是12。这是因为PyTorch使用backward()时默认会累加梯度，需要手动把前一次的梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6., grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "z.backward(create_graph=True)\n",
    "x.grad.data.zero_()\n",
    "x.grad.backward()\n",
    "\n",
    "print(x.grad)\n",
    "# >>>tensor(6., grad_fn=<CopyBackwards>)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例3 \n",
    "\n",
    "有没有发现前面都是对标量求导，如果不是标量会怎么样呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9372/668625584.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;31m# >>>RuntimeError: grad can be implicitly created only for scalar outputs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Users\\yuanz\\anaconda3\\envs\\mynet\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    485\u001B[0m                 \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    486\u001B[0m             )\n\u001B[1;32m--> 487\u001B[1;33m         torch.autograd.backward(\n\u001B[0m\u001B[0;32m    488\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    489\u001B[0m         )\n",
      "\u001B[1;32mc:\\Users\\yuanz\\anaconda3\\envs\\mynet\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    189\u001B[0m     \u001B[0mgrad_tensors_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_tensor_or_tensors_to_tuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad_tensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 190\u001B[1;33m     \u001B[0mgrad_tensors_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_make_grads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mis_grads_batched\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    191\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mretain_graph\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    192\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Users\\yuanz\\anaconda3\\envs\\mynet\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36m_make_grads\u001B[1;34m(outputs, grads, is_grads_batched)\u001B[0m\n\u001B[0;32m     83\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequires_grad\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     84\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mout\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 85\u001B[1;33m                     \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"grad can be implicitly created only for scalar outputs\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     86\u001B[0m                 \u001B[0mnew_grads\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mones_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmemory_format\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreserve_format\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     87\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x + 1\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "# >>>RuntimeError: grad can be implicitly created only for scalar outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "报错了，因为只能标量对标量，标量对向量求梯度，x可以是标量或者向量，但y只能是标量；所以只需要先将y转变为标量，对分别求导没影响的就是求和。\n",
    "此时$x = | x_1, x_2|, y = | {x_1^2},{x_2^2}|, z = y.sum() = {x_1^2} + {x_2^2},$\n",
    "\n",
    "$\\partial z / \\partial x_1 = 2 x_1 = 2, \\partial z / \\partial x_2 = 2 x_2 = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x * x\n",
    "\n",
    "y.sum().backward()\n",
    "print(x.grad)\n",
    "# >>>tensor([2., 4.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再具体一点来解释，让我们写出求导计算的雅可比矩阵，$y = [y_1, y_2]$是一个向量，\n",
    "$J = [\\partial y / \\partial x_1, \\partial y / \\partial x_2] = \n",
    "\\begin{bmatrix}\n",
    "\\frac {\\partial y_1} {\\partial x_1} & \\frac {\\partial y_1} {\\partial x_2}  \\\\\n",
    "\\frac {\\partial y_2} {\\partial x_1} & \\frac {\\partial y_2} {\\partial x_21}  \\\\\n",
    "\\end{bmatrix}\n",
    "$,但是我们希望最终求导的结果是$[\\partial y / \\partial x_1, \\partial y / \\partial x_2]$,那么怎么得到呢？\n",
    "\n",
    "注意$\\frac {\\partial y_1} {\\partial x_2}  $ 和$\\frac {\\partial y_2} {\\partial x_1}  $ = 0,也就是说\n",
    "$\n",
    "[\\partial y / \\partial x_1, \\partial y / \\partial x_2 ] = [1,1]\\begin{bmatrix}\n",
    "\\frac {\\partial y_1} {\\partial x_1} & \\frac {\\partial y_1} {\\partial x_2}  \\\\\n",
    "\\frac {\\partial y_2} {\\partial x_1} & \\frac {\\partial y_2} {\\partial x_21}  \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "所以不用y.sum()的另一种方式是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x * x\n",
    "\n",
    "y.backward(torch.ones_like(y))\n",
    "print(x.grad)\n",
    "# >>>tensor([2., 4.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以使用autograd。上面和这里的torch.ones_like(y) 位置指的就是雅可比矩阵左乘的那个向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x * x\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))\n",
    "print(grad_x[0])\n",
    "# >>>tensor([2., 4.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "y = x * x\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=y.sum(), inputs=x)\n",
    "print(grad_x[0])\n",
    "# >>>tensor([2., 4.])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重点\n",
    "下面是着重强调以及引申的几点：\n",
    "\n",
    "## 梯度清零\n",
    "Pytorch 的自动求导梯度不会自动清零，会累积，所以一次反向传播后需要手动清零\n",
    "\n",
    "```\n",
    "x.grad.zero_\n",
    "```\n",
    "而在神经网络中，我们只需要执行\n",
    "```\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "## 使用detach()切断\n",
    "不会再往后计算梯度，假设有模型A和模型B，我们需要将A的输出作为B的输入，但训练时我们只训练模型B，那么可以这样做：\n",
    "```\n",
    "input_B = output_A.detach()\n",
    "```\n",
    "如果还是以前面的为例子，将a切断，将只有b一条通路，且a变为叶子节点。\n",
    "```\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(x, 1).detach()\n",
    "b = torch.add(x, 2)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"requires_grad: \", x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad)\n",
    "print(\"is_leaf: \", x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf)\n",
    "print(\"grad: \", x.grad, a.grad, b.grad, y.grad)\n",
    "\n",
    ">>>requires_grad:  True False True True\n",
    ">>>is_leaf:  True True False False\n",
    ">>>grad:  tensor([3.]) None None None\n",
    "```\n",
    "## 原位操作 in-place\n",
    "（改变值，不改变对象地址）\n",
    "\n",
    "> 叶子节点不可执行 in-place 操作，因为反向传播时会访问原来的对象地址。\n",
    "\n",
    "\n",
    "# 总结\n",
    "本篇文章其实不难，但是很多人其实会有点糊涂。主要是两点：\n",
    "1. 还记得大学学的链式法则么？如果记得，并不难。\n",
    "2. 了解一下pytorch的技巧即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de19442c968d17280c64ed1ba702ffeb19af57d32723df55799335d91533111e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
